{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Tutorial\n",
    "\n",
    "Written by [Robert Morgan](https://rmorgan10.github.io/), and based on content from [Brian Nord](http://briandnord.com/), [Joshua Yao-Yu Lin](https://joshualincosmo.wordpress.com/), and [Sebastian Raschka](https://sebastianraschka.com/).\n",
    "\n",
    "Let's make a CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pycm import ConfusionMatrix\n",
    "from scipy import misc\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter `pytorch`. PyTorch will be the main workhorse behind our deep learning adventure. Some people use Keras and Tensorflow, but that's not what we'll cover in this tutorial.\n",
    "\n",
    "I chose PyTorch because I believe it provides a lot of functionality that Keras does not. Keras was designed to act as an outer shell for the main deep learning engine tensorflow. While this made the utilities of tensorflow more accessible, it makes it difficult to intorduce sophisticated neural network architectures because you are not interacting directly with tensorflow. PyTorch allows you to have full control over your neural network while (in my opinion) being much easier to interact with than tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in some toy data. The MNIST dataset is a collection of 60,000 labelled, hand-written digits in 28x28 arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('CNN_Tutorial_data/'):\n",
    "    os.mkdir('CNN_Tutorial_data/')\n",
    "    \n",
    "if not os.path.exists('CNN_Tutorial_data/mnist/'):\n",
    "    os.mkdir('CNN_Tutorial_data/mnist')\n",
    "    DOWNLOAD_MNIST = True\n",
    "else:\n",
    "    DOWNLOAD_MNIST = False\n",
    "    \n",
    "data = torchvision.datasets.MNIST(root='CNN_Tutorial_data/mnist', \n",
    "                                  transform=torchvision.transforms.ToTensor(),\n",
    "                                  download=DOWNLOAD_MNIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did that do?**\n",
    "\n",
    "In the top half of the cell, we make sure directories exist for storing the MNIST dataset on your computer and determine if the dataset needs to be downloaded or if it already exists.\n",
    "\n",
    "In the bottom half of the cell, we load the MNIST dataset into the variable `data`. The data is stored in the `root` directory, so in our case this is `CNN_Tutorial_data/mnist/`. The `transform` argument converts the input data format to a `Tensor` object, which is PyTorch's way of processing things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the MNIST dataset\n",
    "\n",
    "First, let's take a look at the data so we know what we are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information stored in `data` has the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in dir(data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like our data and their labels are likely in `data.train_data` and `data.train_labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that guess, let's look at the shapes of these things, and maybe plot a couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.train_data.shape)\n",
    "print(data.train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,5, figsize=(18, 5))\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    axs[i].imshow(data.train_data[i], cmap='gray')\n",
    "    axs[i].set_title('MNIST Label: {0}'.format(data.train_labels[i]), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With images, it's also a good idea to look at the actual pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinda neat how you can almost detect the pattern in the raw pixel values. Anyways, the individual pixels are valued between 0 and 255, and appear to be `int`s. The neural net we will build will multiply weights by the individual pixel values. Therefore, to avoid the products getting too large or the weights getting very small during training, it is standard practice to noramlize the pixel counts to be `float`s between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, PyTorch can do this for us. We already specified that we wanted `transform=torchvision.transforms.ToTensor()` when importing the data, and this transform tells PyTorch to scale the pixel values to be between 0.0 and 1.0 when training.\n",
    "\n",
    "**Note:** If your normalization scheme does not match between your training and testing sets, your network will perform no better than random guessing. Checking that the noramlization has been done correctly is always worth your time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desining a convolutional neural network\n",
    "\n",
    "Our goal now will be to build a convolutional neural network that can take as input a 28x28 image array and correctly identify the handwritten digit based on patterns in the pixel counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PyTorch way of doing things, we will create a class for the classifier. The start of the object will look like this:\n",
    "\n",
    "```python\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "```\n",
    "\n",
    "But what does all that do? Let's break it down line by line.\n",
    "\n",
    "```python \n",
    "class CNN(nn.Module):\n",
    "``` \n",
    "This defines a new abstract object type `CNN`. Becasue of our definition, a `CNN` object can be thought of a similar to a `list` object or a `numpy.ndarray` object: it's just a framework that python will reconize and be able to store data in and work with. Adding `nn.Module` to the argument of the `CNN` object makes the object aware of the properties of `nn.Module`, which remember we imported from PyTorch.\n",
    "\n",
    "```python\n",
    "    def __init__(self):\n",
    "```\n",
    "The `__init__()` function is always the first line of every class. It is automatically executed when ever a `CNN` object is instantiated. It needs the self argument to access, the properties and attributes of the class you're defining, so always add that in.\n",
    "\n",
    "```python\n",
    "        super(CNN, self).__init__()\n",
    "```\n",
    "The `super()` function is a python built-in function. It takes whatever properties, methods, and attributes are defined in the arguments of `CNN` and adds them to `CNN` itself. In our case, the only argument of `CNN` is `nn.Module`, so all methods in `nn.Module` will now be a part of our `CNN` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "If that crash course in object-oriented programming makes sense, then we can move on to using the `CNN` object to define the components and flow of our convolutional neural network. The next step is to define the internal components of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #Network Components\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=32, \n",
    "                               kernel_size=5, \n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=3, \n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=14400, \n",
    "                             out_features=128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=128, \n",
    "                             out_feature=10)\n",
    "\n",
    "```\n",
    "There's a lot in there. Let's unpack it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layers**\n",
    "\n",
    "Convolutional layers are designed to create abstract representations of array-based data. They operate by applying kernels (sometimes called filters, which is super confusing when we try to use CNNs in astronomy), which are matrices of weights, to different parts of the image. Let's look at the convolutional layer illustration below to understand what some of the parameters mean.\n",
    "\n",
    "<img src=\"./images/padding.png\" alt=\"ljag\" width=\"800\"/>\n",
    "\n",
    "[Image Source](https://medium.com/@pushpanjalipd/conventional-neural-network-deep-learning-a2ad1cf4487a)\n",
    "\n",
    "`kernel_size`: In the above figure, `kernel_size`=2. This refers to the size of the blue matrix that is slid across the image, which is 2x2. Generally, you want the kernel size to reflect the size of the features in the image you are trying to detect. It is also useful to use a slightly smaller `kernel_size` in subsequent convolutional layers, since you will be extracting features from the products of the previous convolutional operator.\n",
    "\n",
    "`stride`: Stride is the number of pixels the kernel is slid each time it is applied to the image. In our network, `stride`=1, so the kernels will move one pixel at a time.\n",
    "\n",
    "`padding`: The padding in the above figure is set to 1, and is shown by the yellow boxes with 0 pixel counts. These are not part of the raw image, they are added onto the image by the CNN. Why? Well, let's think about the left border of the image. Without an added column of zeros, the right half of the kernel cannot access the pixel counts in the left border since the left half of the kernel would be outside the image. Adding a border of zeros allows your kernels to sample the entire image. The chosen witdth of the padding should take into account the `kernel_size`, `stride`, and image dimensions.\n",
    "\n",
    "`in_channels`: This parameter refers to the number of filters feeding into the current convolutional layer. If it is the first layer in the network, the `in_channels` parameter should reflect the number of colors. For example, the MNIST dataset is grayscale, so we set `in_channels`=1. In astronomy, we have color information from optical bandpasses ($g$, $r$, $i$, $z$ for DECam), so the first layer may have `in_channels`=4. If this layer is not the first layer and is receiving input from a previous convolutional layer, set the number of `in_channels` to match the previous layer's number of `out_channels`.\n",
    "\n",
    "`out_channels`: This parameter determines how many different convolutional kernels get applied. With low `kernel_size`, the different operators can become redundant. The chosen number of `out_channels` is usually tuned by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout Layers**\n",
    "\n",
    "Neural networks have a lot of parameters. A lot. If you look back at the `CNN` class, you can see that at one point, the network will have 14400 features! If the number of features begins to approach or exceeds the number of pixels in your images, you likely will want to consider a procedure known as dropout.\n",
    "\n",
    "Dropout gives each internal connection in your network a probability (the `p` argument in the layer definition) of being removed from the network. By applying this probability randomly, the chances of the network being able to memorize images instead of lock on to real identifying features is reduced significantly. The machine learning way to phrase this concept is a means to \"reduce overfitting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully-Connected Layers**\n",
    "\n",
    "At some point, we have to go from abstracted representations of the image arrays to probabilities of each class. This is the goal of fully-connected layers.\n",
    "\n",
    "The first step is to flatten whatever array your images are in. You can do this easily with the `torch.flatten()` method. As an example, this operation will take a 30x30 image array and convert to a length 900 vector. Your next goal is to reduce this length 900 vector to **a length equal to the number of classes in your problem**. This choice is made so that the network can straightforwardly compare its predictions to your data labels. For the MNSIT dataset we want to end up with 10 features, one for each class of hand-written digit.\n",
    "\n",
    "You may also notice that in the network we do not perform the compression from 14400 features to 10 features in a single layer. As a general rule of thumb, a fully-connected layer performs well if its output size is roughly the square root of the input, but the number of fully-connected layers and their sizes is certainly something that can and should be tuned by hand to optimize your network.\n",
    "\n",
    "One final important quesiton: how the heck did we get to 14400 features, and how do you know how many interal features your network will have? We'll answer this question in the next section where we look at the flow of data through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "The final portion of network design involves telling the network how to order the layers. This is done by adding a `forward()` method to the `CNN` class. This method should always be named `forward`, unless you choose to do some weird recurrent-like structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #Network Components\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=32, \n",
    "                               kernel_size=5, \n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=3, \n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=14400, \n",
    "                             out_features=128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=128, \n",
    "                             out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Network Flow\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the `forward()` method. You can see that in each line, the infomation carried in the variable `x` is passed through one of the layers defined in the `CNN` object, and then the output of that layer is stored in `x` again and then sent to the next layer. As a technical note, the computations performed while training require a lot of memory on your CPU. To lighten the load a little bit, we overwrite the same variable so that a new memory address is not required for each layer output.\n",
    "\n",
    "You will also notice there are some new items here: `max_pool2d`, `relu`, and `log_softmax`. `relu` and `softmax` are the names of activation functions. You can read about them [here](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6). They are internal steps the network uses to determine whether a given abstract representation is of value, and they act in a way that turns on and off parts of the network based on the parts' importance in obtaining the right classificaiton.\n",
    "\n",
    "`max_pool2d`: This type of layer is used to reduce the overall size of the images by looking at a section of an image, determining which part of it carries the most weight in making classifications, and then using only that part to represent the entire section. Here is an illustration to make that point clear:\n",
    "\n",
    "<img src=\"./images/pooling.png\" alt=\"ljag\" width=\"600\"/>\n",
    "\n",
    "[Image Source](https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L13_intro-cnn/L13_intro-cnn-part1_slides.pdf)\n",
    "\n",
    "In practice, `max_pool2d` layers tend to follow sets of convolutional layers, since they help to choose the most significant of the abstracted features produced during the convolution.\n",
    "\n",
    "---\n",
    "\n",
    "Let's briefly return to the issue of tracking the number of features through the network. And let's look at our network as a test case. We started with images of dimension 28x28. This is already 784 features, one for each pixel.\n",
    "\n",
    "- Step 1: We applied a convolutional layer with `kernel_size`=5, `stride`=1, and `padding`=2. After a convolution with these properties, the new image widths are given by: $$ \\textrm{New Width} = \\frac{\\textrm{Old Width} - \\textrm{Kernel Width} + 2 \\times \\textrm{Padding}}{\\textrm{Stride}} + 1,$$ so in our case the new image dimensions are 28x28. We are still at 784 features.\n",
    "\n",
    "- Step 2: We apply a `relu` activation function, but this just applies an operation to each feature without combining any or producing any new ones, so we remain at 784 features.\n",
    "\n",
    "- Step 3: We apply a second convolutional layer, this time with `kernel_size`=3, `stride`=1, and `padding`=2. Using the above formula, the new image widths will be 30x30, so we now have 900 features.\n",
    "\n",
    "- Step 4: Time for `max_pool2d` with `kernel_size=2`, which compresses the images but does so by adding more weights to the network. Specifically, for each pixel in the image width, you apply a 2x2 kernel. So for an image of width 30, you are effectively lengthing it to $30 \\times 2 \\times 2$ = 120. Meaning as arrays we have 120x120 images, or 14400 features.\n",
    "\n",
    "- Step 5: Dropout layers operate by removing individual connections at random, not by introducing new weights, so no new features are introduced here.\n",
    "\n",
    "- Steps 6+: We now flatten the data arrays. So our 120x120 arrays are officially converted to length 14400 vectors and are ready for the fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have looked at all the individual components of our neural network, lets put it all together in a cell that can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #Network Components\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=32, \n",
    "                               kernel_size=5, \n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=3, \n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=14400, \n",
    "                             out_features=128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=128, \n",
    "                             out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Network Flow\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Now we have a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to tell the neural network how to learn. We do this by specifying a loss function. While training, the neural network will make predictions about the input data and compare those predictions to the true labels of the data. If the neural network gets the answer wrong, then the interanl parameters of the neural network that led to the incorrect decision need to be updated. The amount that the interal parameters (weights) are updated after a wrong decision is what the loss function specifies. A standard loss function is catagorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function defines a surface in a very high dimensional space--the space will have the same number of dimensions as the neural network has weights, so this can easily get to hundreds of thousands. Not surprisingly, figuring out which direction to move in this complicated space to update the weights can be computationally expensive. Thankfully, smart people have developed algorithms for moving through this space in efficient ways. To specify how you move along the loss surface in the space of the weights, you use something called an optimizer. The Adam optimizer is an industry standard.\n",
    "\n",
    "Optimizers typically require some input parameters that tell them how fast to move along the loss surface. For the Adam optimizer, we have to specify something called the learning rate. Large learning rates mean big steps along the loss surface, so you will approach the optimal solution faster but risk overshooting it. Small learning rates mean tiny steps along the loss surface, so your overall training time will be slower, and as well you also risk getting stuck in a local minimum of the loss surface. In practice, this is a parameter you should tune by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train the neural network!\n",
    "\n",
    "Two things we need to specify are how long we want to allow the neural network to train and how many images we want it to look at simultaneously. An epoch is how many times you want the network to find a minimum in the loss surface, adjust the weights to that position, and then start the trianing process over. Since time spent training the network is real time in your life, it would be ideal if this happened as quickly as possible, but too few training epochs and you may not reach the most optimal network configuration. A similar argument can be made for batch size, which is the number of images the network sees at once. If your batch size is too small, the weights may start to memorize the individual images in each batch rather than lock on to more global features, but too many images at once can make it difficult for the network to isolate the most informative features. Again, in practice, both of these parameters are tuned by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_epochs = 3\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a good machine learning habbit, we should train our network on our training data, but evaluate it on unseen validation data. Our training data is ready to go, but we'll have to bring in and preprocess some new validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.MNIST(root='CNN_Tutorial_data/mnist/', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[0:1000]\n",
    "test_y = test_data.test_labels[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to prepare the training data to be loaded into the network in batches. In PyTorch, this process is done using a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training, we'll print out the network accuracy on small subsets of the training and testing data. If the performance is significantly different between these two datasets, or if the network performs at the random guessing level, scrutinize every detail of how you did the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the training below, the network will pause the training to make predictions every 10 steps for the purpose of nice(-ish) looking diagnostic plots later on. Making predictions every 10 steps slows the network down considerably. If you want to get through this step faster, change \n",
    "\n",
    "```python\n",
    "if step % 10 == 0:\n",
    "``` \n",
    "\n",
    "to \n",
    "```python\n",
    "if step % 100 == 0:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S GOOOO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append some lists as we go to track the accuracies and loss\n",
    "indices, losses, train_acc, validation_acc = [], [], [], []\n",
    "index_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(number_of_training_epochs):\n",
    "    \n",
    "    for step, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear out all existing gradients on the loss surface to reevaluate for this step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Get the CNN's current prediction of the training data\n",
    "        output = cnn(batch_data)\n",
    "        \n",
    "        #Calculate the loss by comparing the prediction to the truth\n",
    "        loss = loss_function(output, batch_labels) \n",
    "        \n",
    "        #Evaluate all gradients along the loss surface using back propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        #Based on the gradients, take the optimal step in the weight space\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Every so often, let's print out the accuracy\n",
    "        if step % 1000 == 0:\n",
    "            \n",
    "            #Evaluate the network's predictions\n",
    "            train_output = cnn(torch.unsqueeze(data.train_data, dim=1).type(torch.FloatTensor)[0:200])\n",
    "            validation_output = cnn(test_x)\n",
    "            \n",
    "            train_predictions = torch.max(train_output, 1)[1].data.numpy()\n",
    "            validation_predictions = torch.max(validation_output, 1)[1].data.numpy()\n",
    "            \n",
    "            #Calculate accuracy\n",
    "            train_accuracy = float((train_predictions == data.train_labels.numpy()[0:200]).astype(int).sum()) / float(train_predictions.size)\n",
    "            validation_accuracy = float((validation_predictions == test_y.data.numpy()).astype(int).sum()) / float(validation_predictions.size)\n",
    "            \n",
    "            print(\"Epoch: {0} Step: {1}  | Training Accuracy: {2} -- Validation Accuracy: {3}\".format(epoch + 1, step, train_accuracy, validation_accuracy))\n",
    "            \n",
    "            #save results to list for diagnostic plots\n",
    "            indices.append(index_counter)\n",
    "            losses.append(loss.data.numpy())\n",
    "            train_acc.append(train_accuracy)\n",
    "            validation_acc.append(validation_accuracy)\n",
    "            index_counter += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Some common ways to assess the performance of you network.\n",
    "\n",
    "**Method 1**: Plot out the images with the label predicted for them by the CNN. This should give you an intuition for what features the CNN is basing the classifications on and what features it may be missing. If you notice a pattern in features that are not being picked up on (for example, it misses discriminating features that are similar size), consider adjusting the `kernel_sizes` in your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "counter = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axs[i, j].imshow(test_data.test_data[counter], cmap='gray')\n",
    "        axs[i, j].set_title('CNN Prediction: {0}'.format(validation_predictions[counter]), fontsize=22)\n",
    "        counter += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are doing pretty well, but there are a couple images confusing the network. Not surprisingly, these hand-written digits look a little off, so the network should be tuned to get these right. A worthwhile exercise is to augment the above network, restart the notebook kernel, and retrain until you see improvements in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: Assessing loss and accuracy as functions of training time. As you may have noticed above, training took a while, but the accuracy seemed to stagnate after a little while. This behavior indicates that we might as well save ourselves some time and stop the training earlier. Here are some plots to visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(indices, losses, train_acc, validation_acc, number_of_training_epochs):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    fit = np.poly1d(np.polyfit(indices, losses, 50))(indices)\n",
    "    ax1.plot(indices, losses, color='darkgreen', alpha=0.3)\n",
    "    ax1.plot(indices, fit, color='darkgreen', lw=2)\n",
    "    ax1.set_xlabel(\"Training Step\", fontsize=22)\n",
    "    ax1.set_ylabel(\"Categorical Cross Entropy Loss\", fontsize=22)\n",
    "    for ii in range(number_of_training_epochs):\n",
    "        xval = (ii + 1) / number_of_training_epochs * np.max(indices)\n",
    "        ax1.axvline(x=xval, ls='--', color='black')\n",
    "        ax1.text(xval - number_of_training_epochs, 0.9, \"Epoch {0}\".format(ii + 1), horizontalalignment='right', fontsize=18)\n",
    "    ax1.set_ylim(0.0, 1.0)\n",
    "\n",
    "    train_fit = np.poly1d(np.polyfit(indices, train_acc, 50))(indices)\n",
    "    validation_fit = np.poly1d(np.polyfit(indices, validation_acc, 50))(indices)\n",
    "    ax2.plot(indices, train_acc, alpha=0.3, color='darkorange')\n",
    "    ax2.plot(indices, validation_acc, alpha=0.3, color='darkblue')\n",
    "    ax2.plot(indices, train_fit, lw=2, color='darkorange', label=\"Training Data\")\n",
    "    ax2.plot(indices, validation_fit, lw=2, color='darkblue', label=\"Validation Data\")\n",
    "    ax2.legend(loc='center right', fontsize=22)\n",
    "    ax2.set_xlabel(\"Training Step\", fontsize=22)\n",
    "    ax2.set_ylabel(\"Accuracy\", fontsize=22)\n",
    "    for ii in range(number_of_training_epochs):\n",
    "        xval = (ii + 1) / number_of_training_epochs * np.max(indices)\n",
    "        ax2.axvline(x=xval, ls='--', color='black')\n",
    "        ax2.text(xval - number_of_training_epochs, 0.82, \"Epoch {0}\".format(ii + 1), horizontalalignment='right', fontsize=18)\n",
    "    ax2.set_ylim(0.8, 1.0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_accuracy(indices, losses, train_acc, validation_acc, number_of_training_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left we plot the loss, which is the quantity the network is attempting to minimize while making its classifications. On the right we evaluate the accuracy on the training and validation sets. The noise level in these curves is normal behavior, as there is a lot of randomness going on under the hood. It is also a good sign that the training and validation accuracies trace each other. If this were not the case, we might start to worry that either our training data was not representative of all the aspects of our validation data, or that the network was overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MNIST dataset, it seems like all of these curves level off after the second epoch, so when tweaking the network, you might consider just terminating the training after two epochs to save yourself some time. The MNIST dataset is very easy to work with though, so in practice you will need many more epochs. As well, we only had 4 total layers in this network; the deeper you make your network, the more epochs your network will require to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 3**: Confusion matrices. The plots below are one of your best tools for assessing points of confusion in your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_index(array_categorical):\n",
    "    array_index = [np.argmax(array_temp) for array_temp in array_categorical]\n",
    "    print(np.unique(array_index))\n",
    "    return array_index\n",
    "\n",
    "def plot_confusion_matrix(cm, ax,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function modified to plots the ConfusionMatrix object.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Code Reference : \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This script is derived from PyCM repository: https://github.com/sepandhaghighi/pycm\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    plt_cm = []\n",
    "    for i in cm.classes :\n",
    "        row=[]\n",
    "        for j in cm.classes:\n",
    "            row.append(cm.table[i][j])\n",
    "        plt_cm.append(row)\n",
    "    plt_cm = np.array(plt_cm)\n",
    "    if normalize:\n",
    "        plt_cm = plt_cm.astype('float') / plt_cm.sum(axis=1)[:, np.newaxis]     \n",
    "    ax.imshow(plt_cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title, fontsize=28)\n",
    "    #ax.colorbar()\n",
    "    tick_marks = np.arange(len(cm.classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels([str(x) for x in cm.classes], fontsize=20)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels([str(x) for x in cm.classes], fontsize=20)\n",
    "    \n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = plt_cm.max() / 2.\n",
    "    for i, j in itertools.product(range(plt_cm.shape[0]), range(plt_cm.shape[1])):\n",
    "        ax.text(j, i, format(plt_cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if plt_cm[i, j] > thresh else \"black\",\n",
    "                fontsize=16)\n",
    "\n",
    "    \n",
    "    ax.set_ylabel('Actual', fontsize=24)\n",
    "    ax.set_xlabel('Predict', fontsize=24)\n",
    "    return ax\n",
    "\n",
    "def plot_heatmaps(training_predictions, validation_predictions, training_labels, validation_labels):\n",
    "    train_true_idx = training_labels.numpy()\n",
    "    train_pred_idx = training_predictions\n",
    "    validation_true_idx = validation_labels.numpy()\n",
    "    validation_pred_idx = validation_predictions\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    cm_train = ConfusionMatrix(train_true_idx, train_pred_idx)\n",
    "    ax1 = plot_confusion_matrix(cm_train, ax1, title=\"Training Data\")\n",
    "    \n",
    "    cm_validation = ConfusionMatrix(validation_true_idx, validation_pred_idx)\n",
    "    ax2 = plot_confusion_matrix(cm_validation, ax2, title=\"Validation Data\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below evaluates the predictions on the full training and validaion sets. It may take a minute or two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting all training set predictions\")\n",
    "training_predictions = torch.max(cnn(torch.unsqueeze(data.train_data, dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()\n",
    "print(\"Getting all validation set predictions\")\n",
    "validation_predictions = torch.max(cnn(torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps(training_predictions, validation_predictions, data.train_labels, test_data.test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this network performs very well, evidenced by the prominent main diagonal in these matrices. There is notable confusion between 9 and 4, judging by the largest off-diagonal elements. We might consider starting the convolutional layers with a larger kernel, since the top of the 9 (which is the only real difference between a 9 and a 4) is typically larger than 5 pixels wide in these images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaknesses of CNNs\n",
    "\n",
    "Convolution is not a rotation-, scale-, or translation-invariant operation.\n",
    "\n",
    "The reason for this fun-to-deal-with property is the convolution kernels operate on local features. See the example below, which is illustrating translation confusing a CNN.\n",
    "\n",
    "<img src=\"./images/invariance.png\" alt=\"ljag\" width=\"600\"/>\n",
    "\n",
    "[Image Source](https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L13_intro-cnn/L13_intro-cnn-part1_slides.pdf)\n",
    "\n",
    "In the convolved representation of this image of the number 5, different regions are being activated, which leads to different information being passed through the network even though this is the same real object. You can imagine that the network would be confused in similar fashion by different sized numbers and rotated numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a fun test of this weakness, let's try to confuse our CNN beyond belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can our CNN handle rotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotation using scipy.ndimage.rotate will expand the image by filling in zeros, so we need to trim it back down\n",
    "def zoom_after_rotation(x, desired_shape=(28, 28)):\n",
    "    current_width = np.shape(x)[0]\n",
    "    assert current_width > desired_shape[0]\n",
    "    num_to_trim = int(round((np.shape(x)[0] - desired_shape[0]) / 2))\n",
    "    return x[num_to_trim: - num_to_trim, num_to_trim: - num_to_trim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotate images in test set\n",
    "rotated_test_images_30_deg_expanded = [ndimage.rotate(x, angle=30.0) for x in test_data.test_data[0:5]]\n",
    "rotated_test_images_30_deg = np.array([zoom_after_rotation(x)/255. for x in rotated_test_images_30_deg_expanded])\n",
    "\n",
    "rotated_test_images_90_deg = np.array([ndimage.rotate(x, angle=90.0) for x in test_data.test_data[0:5]])\n",
    "\n",
    "rotated_test_images_180_deg = np.array([ndimage.rotate(x, angle=180.0) for x in test_data.test_data[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get CNN predictions on rotated images\n",
    "rotated_test_images_30_deg_predictions = torch.max(cnn(torch.unsqueeze(torch.from_numpy(rotated_test_images_30_deg), dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()\n",
    "rotated_test_images_90_deg_predictions = torch.max(cnn(torch.unsqueeze(torch.from_numpy(rotated_test_images_90_deg), dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()\n",
    "rotated_test_images_180_deg_predictions = torch.max(cnn(torch.unsqueeze(torch.from_numpy(rotated_test_images_180_deg), dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results\n",
    "images = [rotated_test_images_30_deg, rotated_test_images_90_deg, rotated_test_images_180_deg]\n",
    "preds = [rotated_test_images_30_deg_predictions, rotated_test_images_90_deg_predictions, rotated_test_images_180_deg_predictions]\n",
    "titles = [\"30 Degree Rotation\", \"90 Degree Rotation\", \"180 Degree Rotation\"]\n",
    "for ii in range(3):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    for jj in range(5):\n",
    "        axs[jj].imshow(images[ii][jj], cmap='gray')\n",
    "        axs[jj].set_xlabel(\"CNN Prediction: {0}\".format(preds[ii][jj]), fontsize=20)\n",
    "    fig.suptitle(titles[ii], fontsize=24)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly, a CNN cannot handle rotated images if it was not exposed to them during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about changes to the scale of the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom out on images in test set\n",
    "zoom_out_images = np.array([misc.imresize(np.pad(x, 14, mode='minimum'), (28,28)) for x in test_data.test_data[0:5]])\n",
    "\n",
    "# Zoom in on images in test set\n",
    "zoom_in_images = np.array([misc.imresize(x[6:-5, 6:-5], (28,28)) for x in test_data.test_data[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from CNN\n",
    "zoom_out_pred = torch.max(cnn(torch.unsqueeze(torch.from_numpy(zoom_out_images), dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()\n",
    "zoom_in_pred = torch.max(cnn(torch.unsqueeze(torch.from_numpy(zoom_in_images), dim=1).type(torch.FloatTensor)), 1)[1].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "images = [zoom_out_images, zoom_in_images]\n",
    "preds = [zoom_out_pred, zoom_in_pred]\n",
    "titles = [\"Zoomed Out\", \"Zoomed In\"]\n",
    "for ii in range(2):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    for jj in range(5):\n",
    "        axs[jj].imshow(images[ii][jj], cmap='gray')\n",
    "        axs[jj].set_xlabel(\"CNN Prediction: {0}\".format(preds[ii][jj]), fontsize=20)\n",
    "    fig.suptitle(titles[ii], fontsize=24)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly the CNN is sensitive to scale of the objects in the images, as the size of the features it can pick up is dependent on both the features sizes in the training set and the user-specified kernel dimensions. \n",
    "\n",
    "For rotations, scale changes, and translations, the problem can be approached from a preprossing standpoint. The preprocessing approach is to augment the training set to include rotated images, translated images, zoomed in or out images, or even partial images. This step will allow the network to recognize the same object in multiple representations and is an essential first step of most real world applications of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we\n",
    "\n",
    "- Saw how to work with PyTorch\n",
    "- Built a CNN\n",
    "- Trained our CNN using a CPU\n",
    "- Assessed the performance of our CNN\n",
    "- Found weaknesses in the CNN approach\n",
    "\n",
    "### Topics for later tutorials\n",
    "\n",
    "- Save a trained model\n",
    "- Utilize a GPU\n",
    "- Batch normalization\n",
    "- Autoencoders\n",
    "- Multicolor image classification\n",
    "- Regression with CNNs\n",
    "- Advanced preprocessing\n",
    "    - cropping\n",
    "    - centering\n",
    "    - contour detection\n",
    "    - image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
